Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Min2005b,
abstract = {Bankruptcy prediction has drawn a lot of research interests in previous literature, and recent studies have shown that machine learning techniques achieved better performance than traditional statistical ones. This paper applies support vector machines (SVMs) to the bankruptcy prediction problem in an attempt to suggest a new model with better explanatory power and stability. To serve this purpose, we use a grid-search technique using 5-fold cross-validation to find out the optimal parameter values of kernel function of SVM. In addition, to evaluate the prediction accuracy of SVM, we compare its performance with those of multiple discriminant analysis (MDA), logistic regression analysis (Logit), and three-layer fully connected back-propagation neural networks (BPNs). The experiment results show that SVM outperforms the other methods. ?? 2005 Published by Elsevier Ltd.},
author = {Min, Jae H. and Lee, Young Chan},
doi = {10.1016/j.eswa.2004.12.008},
file = {:E$\backslash$:/paper/mendeley/Min, Lee - 2005 - Bankruptcy prediction using support vector machine with optimal choice of kernel function parameters.pdf:pdf},
isbn = {09574174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Back-propagation neural networks,Bankruptcy prediction,Grid-search,Kernel function,Support vector machine},
number = {4},
pages = {603--614},
title = {{Bankruptcy prediction using support vector machine with optimal choice of kernel function parameters}},
volume = {28},
year = {2005}
}
@article{Min2006b,
abstract = {Bankruptcy prediction is an important and widely studied topic since it can have significant impact on bank lending decisions and profitability. Recently, the support vector machine (SVM) has been applied to the problem of bankruptcy prediction. The SVM-based method has been compared with other methods such as the neural network (NN) and logistic regression, and has shown good results. The genetic algorithm (GA) has been increasingly applied in conjunction with other AI techniques such as NN and Case-based reasoning (CBR). However, few studies have dealt with the integration of GA and SVM, though there is a great potential for useful applications in this area. This study proposes methods for improvingSVMperformance in two aspects: feature subset selection and parameter optimization.GAis used to optimize both a feature subset and parameters of SVM simultaneously for bankruptcy prediction.},
author = {Min, Sung-Hwan and Lee, Jumin and Han, Ingoo},
doi = {10.1016/j.eswa.2005.09.070},
file = {:E$\backslash$:/paper/mendeley/Min, Lee, Han - 2006 - Hybrid genetic algorithms and support vector machines for bankruptcy prediction.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {bankruptcy prediction,genetic algorithms,support vector machines},
number = {3},
pages = {652--660},
title = {{Hybrid genetic algorithms and support vector machines for bankruptcy prediction}},
volume = {31},
year = {2006}
}
@article{Chauhan2009b,
abstract = {In this study, differential evolution algorithm (DE) is proposed to train a wavelet neural network (WNN). The resulting network is named as differential evolution trained wavelet neural network (DEWNN). The efficacy of DEWNN is tested on bankruptcy prediction datasets viz. US banks, Turkish banks and Spanish banks. Further, its efficacy is also tested on benchmark datasets such as Iris, Wine and Wisconsin Breast Cancer. Moreover, Garson's algorithm for feature selection in multi layer perceptron is adapted in the case of DEWNN. The performance of DEWNN is compared with that of threshold accepting trained wavelet neural network (TAWNN) [Vinay Kumar, K., Ravi, V., Mahil Carr, {\&} Raj Kiran, N. (2008). Software cost estimation using wavelet neural networks. Journal of Systems and Software] and the original wavelet neural network (WNN) in the case of all data sets without feature selection and also in the case of four data sets where feature selection was performed. The whole experimentation is conducted using 10-fold cross validation method. Results show that soft computing hybrids viz., DEWNN and TAWNN outperformed the original WNN in terms of accuracy and sensitivity across all problems. Furthermore, DEWNN outscored TAWNN in terms of accuracy and sensitivity across all problems except Turkish banks dataset. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Chauhan, Nikunj and Ravi, V. and {Karthik Chandra}, D.},
doi = {10.1016/j.eswa.2008.09.019},
file = {:E$\backslash$:/paper/mendeley/Chauhan, Ravi, Karthik Chandra - 2009 - Differential evolution trained wavelet neural networks Application to bankruptcy prediction in b.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Bankruptcy prediction,Classification,Differential evolution (DE),Differential evolution trained wavelet neural netw,Threshold accepting trained wavelet neural network,Wavelet neural networks (WNN)},
number = {4},
pages = {7659--7665},
publisher = {Elsevier Ltd},
title = {{Differential evolution trained wavelet neural networks: Application to bankruptcy prediction in banks}},
url = {http://dx.doi.org/10.1016/j.eswa.2008.09.019},
volume = {36},
year = {2009}
}
@article{RaviKumar2007b,
abstract = {This paper presents a comprehensive review of the work done, during the 1968-2005, in the application of statistical and intelligent techniques to solve the bankruptcy prediction problem faced by banks and firms. The review is categorized by taking the type of technique applied to solve this problem as an important dimension. Accordingly, the papers are grouped in the following families of techniques: (i) statistical techniques, (ii) neural networks, (iii) case-based reasoning, (iv) decision trees, (iv) operational research, (v) evolutionary approaches, (vi) rough set based techniques, (vii) other techniques subsuming fuzzy logic, support vector machine and isotonic separation and (viii) soft computing subsuming seamless hybridization of all the above-mentioned techniques. Of particular significance is that in each paper, the review highlights the source of data sets, financial ratios used, country of origin, time line of study and the comparative performance of techniques in terms of prediction accuracy wherever available. The review also lists some important directions for future research. ?? 2006 Elsevier B.V. All rights reserved.},
author = {{Ravi Kumar}, P. and Ravi, V.},
doi = {10.1016/j.ejor.2006.08.043},
file = {:E$\backslash$:/paper/mendeley/Ravi Kumar, Ravi - 2007 - Bankruptcy prediction in banks and firms via statistical and intelligent techniques - A review.pdf:pdf},
isbn = {03772217},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Bankruptcy prediction,Banks,Case-based reasoning,Decision trees,Evolutionary approaches,Firms,Fuzzy logic,Intelligent techniques,Neural networks,Operations research,Rough sets,Statistics,Support vector machine and soft computing},
number = {1},
pages = {1--28},
pmid = {23807594},
title = {{Bankruptcy prediction in banks and firms via statistical and intelligent techniques - A review}},
volume = {180},
year = {2007}
}
@article{Domingos2012b,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
eprint = {9605103},
file = {:E$\backslash$:/paper/mendeley/Domingos - 2012 - A few useful things to know about machine learning(4).pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pages = {78},
pmid = {1000183096},
primaryClass = {cs},
title = {{A few useful things to know about machine learning}},
volume = {55},
year = {2012}
}
@article{Statistics2017b,
author = {Statistics, Mathematical},
file = {:E$\backslash$:/paper/mendeley/Statistics - 2017 - Greedy Function Approximation A Gradient Boosting Machine Author ( s ) Jerome H . Friedman Source The Annals of St.pdf:pdf},
number = {5},
pages = {1189--1232},
title = {{Greedy Function Approximation : A Gradient Boosting Machine Author ( s ): Jerome H . Friedman Source : The Annals of Statistics , Vol . 29 , No . 5 ( Oct ., 2001 ), pp . 1189-1232 Published by : Institute of Mathematical Statistics Stable URL : http://www}},
volume = {29},
year = {2017}
}
@article{Chih-WeiHsuChih-ChungChang2008b,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
file = {:E$\backslash$:/paper/mendeley/Chih-Wei Hsu, Chih-Chung Chang - 2008 - A Practical Guide to Support Vector Classification.pdf:pdf},
isbn = {013805326X},
issn = {1464-410X},
journal = {BJU international},
number = {1},
pages = {1396--400},
pmid = {18190633},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@article{Kim2015b,
abstract = {In classification or prediction tasks, data imbalance problem is frequently observed when most of instances belong to one majority class. Data imbalance problem has received considerable attention in machine learning community because it is one of the main causes that degrade the performance of classifiers or predictors. In this paper, we propose geometric mean based boosting algorithm (GMBoost) to resolve data imbalance problem. GMBoost enables learning with consideration of both majority and minority classes because it uses the geometric mean of both classes in error rate and accuracy calculation. To evaluate the performance of GMBoost, we have applied GMBoost to bankruptcy prediction task. The results and their comparative analysis with AdaBoost and cost-sensitive boosting indicate that GMBoost has the advantages of high prediction power and robust learning capability in imbalanced data as well as balanced data distribution.},
author = {Kim, Myoung Jong and Kang, Dae Ki and Kim, Hong Bae},
doi = {10.1016/j.eswa.2014.08.025},
file = {:E$\backslash$:/paper/mendeley/Kim, Kang, Kim - 2015 - Geometric mean based boosting algorithm with over-sampling to resolve data imbalance problem for bankruptcy pred.pdf:pdf},
isbn = {9781612082479},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {AdaBoost,Bankruptcy prediction,Cost-sensitive boosting,Data imbalance,GMBoost,Over-sampling,SMOTE},
number = {3},
pages = {1074--1082},
publisher = {Elsevier Ltd},
title = {{Geometric mean based boosting algorithm with over-sampling to resolve data imbalance problem for bankruptcy prediction}},
url = {http://dx.doi.org/10.1016/j.eswa.2014.08.025},
volume = {42},
year = {2015}
}
@article{Zhou2013b,
abstract = {Corporate bankruptcy prediction is very important for creditors and investors. Most literature improves performance of prediction models by developing and optimizing the quantitative methods. This paper investigates the effect of sampling methods on the performance of quantitative bankruptcy prediction models on real highly imbalanced dataset. Seven sampling methods and five quantitative models are tested on two real highly imbalanced datasets. A comparison of model performance tested on random paired sample set and real imbalanced sample set is also conducted. The experimental results suggest that the proper sampling method in developing prediction models is mainly dependent on the number of bankruptcies in the training sample set. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Zhou, Ligang},
doi = {10.1016/j.knosys.2012.12.007},
file = {:E$\backslash$:/paper/mendeley/Zhou - 2013 - Performance of corporate bankruptcy prediction models on imbalanced dataset The effect of sampling methods.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Bankruptcy prediction,Classification,Imbalanced dataset,Oversampling,Undersampling},
pages = {16--25},
title = {{Performance of corporate bankruptcy prediction models on imbalanced dataset: The effect of sampling methods}},
url = {http://dx.doi.org/10.1016/j.knosys.2012.12.007},
volume = {41},
year = {2013}
}
@article{Schapire1999b,
abstract = {Boosting is a general method for improving the accuracy of any given learning algorithm. This short overview paper introduces the boosting algorithm AdaBoost, and explains the underlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting's relationship to support-vector machines. Some examples of recent applications of boosting are also described.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.01136v1},
author = {Schapire, Robert E.},
doi = {citeulike-article-id:765005},
eprint = {arXiv:1508.01136v1},
file = {:E$\backslash$:/paper/mendeley/Schapire - 1999 - A brief introduction to boosting.pdf:pdf},
isbn = {3540440119},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
number = {5},
pages = {1401--1406},
title = {{A brief introduction to boosting}},
volume = {2},
year = {1999}
}
@article{He2009b,
abstract = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {He, Haibo and Garcia, Edwardo A.},
doi = {10.1109/TKDE.2008.239},
eprint = {arXiv:1011.1669v3},
file = {:E$\backslash$:/paper/mendeley/He, Garcia - 2009 - Learning from imbalanced data.pdf:pdf},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Active learning,Assessment metrics,Classification,Cost-sensitive learning,Imbalanced learning,Kernel-based learning,Sampling methods},
number = {9},
pages = {1263--1284},
pmid = {24807526},
title = {{Learning from imbalanced data}},
volume = {21},
year = {2009}
}
@article{Guyon2003b,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre- dictors, providing faster andmore cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
doi = {10.1016/j.aca.2011.07.027},
eprint = {1111.6189v1},
file = {:E$\backslash$:/paper/mendeley/Guyon, Elisseeff - 2003 - An Introduction to Variable and Feature Selection(2).pdf:pdf},
isbn = {0885-6125},
issn = {00032670},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Biochemical oxygen demand,Kernel discriminant analysis,Kernel partial least squares,Support vector classification,Support vector regression,Water quality},
number = {3},
pages = {1157--1182},
pmid = {21889629},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@article{Chen2016b,
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end- to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quan- tile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compres- sion and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
archivePrefix = {arXiv},
arxivId = {1603.02754},
author = {Chen, Tianqi and Guestrin, Carlos},
doi = {10.1145/2939672.2939785},
eprint = {1603.02754},
file = {:E$\backslash$:/paper/mendeley/Chen, Guestrin - 2016 - XGBoost Reliable Large-scale Tree Boosting System.pdf:pdf},
isbn = {9781450342322},
issn = {0146-4833},
journal = {arXiv},
keywords = {large-scale machine learning},
pages = {1--6},
title = {{XGBoost : Reliable Large-scale Tree Boosting System}},
year = {2016}
}
@article{Wang2014b,
abstract = {With the recent financial crisis and European debt crisis, corporate bankruptcy prediction has become an increasingly important issue for financial institutions. Many statistical and intelligent methods have been proposed, however, there is no overall best method has been used in predicting corporate bankruptcy. Recent studies suggest ensemble learning methods may have potential applicability in corporate bankruptcy prediction. In this paper, a new and improved Boosting, FS-Boosting, is proposed to predict corporate bankruptcy. Through injecting feature selection strategy into Boosting, FS-Booting can get better performance as base learners in FS-Boosting could get more accuracy and diversity. For the testing and illustration purposes, two real world bankruptcy datasets were selected to demonstrate the effectiveness and feasibility of FS-Boosting. Experimental results reveal that FS-Boosting could be used as an alternative method for the corporate bankruptcy prediction. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Wang, Gang and Ma, Jian and Yang, Shanlin},
doi = {10.1016/j.eswa.2013.09.033},
file = {:E$\backslash$:/paper/mendeley/Wang, Ma, Yang - 2014 - An improved boosting based on feature selection for corporate bankruptcy prediction.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Boosting,Corporate bankruptcy prediction,Ensemble learning,Feature selection},
number = {5},
pages = {2353--2361},
publisher = {Elsevier Ltd},
title = {{An improved boosting based on feature selection for corporate bankruptcy prediction}},
url = {http://dx.doi.org/10.1016/j.eswa.2013.09.033},
volume = {41},
year = {2014}
}
@article{Nanni2009b,
abstract = {In this paper, we investigate the performance of several systems based on ensemble of classifiers for bankruptcy prediction and credit scoring. The obtained results are very encouraging, our results improved the performance obtained using the stand-alone classifiers. We show that the method "Random Subspace" outperforms the other ensemble methods tested in this paper. Moreover, the best stand-alone method is the multi-layer perceptron neural net, while the best method tested in this work is the Random Subspace of Levenberg-Marquardt neural net. In this work, three financial datasets are chosen for the experiments: Australian credit, German credit, and Japanese credit. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Nanni, Loris and Lumini, Alessandra},
doi = {10.1016/j.eswa.2008.01.018},
file = {:E$\backslash$:/paper/mendeley/Nanni, Lumini - 2009 - An experimental comparison of ensemble of classifiers for bankruptcy prediction and credit scoring.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Bankruptcy prediction,Credit scoring,Ensemble of classifiers},
number = {2 PART 2},
pages = {3028--3033},
publisher = {Elsevier Ltd},
title = {{An experimental comparison of ensemble of classifiers for bankruptcy prediction and credit scoring}},
url = {http://dx.doi.org/10.1016/j.eswa.2008.01.018},
volume = {36},
year = {2009}
}
@article{Liu2015b,
abstract = {A multi-label classification based approach for sentiment analysis is proposed in this paper. To the best of our knowledge, this work is the first to propose to use multi-label classification for sentiment classification of microblogs. The proposed prototype has three main components, text segmentation, feature extraction, and multi-label classification. Raw segmented words and sentiment features based on the three different sentiment dictionaries, Dalian University of Technology Sentiment Dictionary, National Taiwan University Sentiment Dictionary and HowNet Dictionary, are the features and the bag of words is the feature representation. A detailed empirical study of different multi-label classification methods on sentiment classification is conducted to compare their classification performances. Specifically, total 11 state of the art multi-label classification methods are compared on two microblog datasets and 8 evaluation metrics are used. The effects of the three sentiment dictionaries for multi-label classification are empirically studied and compared, which, to the best of our knowledge, have not been performed. The performed empirical comparisons show that Dalian University of Technology Sentiment Dictionary has the best performance among the three different sentiment dictionaries.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Liu, Shuhua Monica and Chen, Jiun-Hung},
doi = {10.1016/j.eswa.2014.08.036},
eprint = {arXiv:1011.1669v3},
file = {:E$\backslash$:/paper/mendeley/Liu, Chen - 2015 - A multi-label classification based approach for sentiment classification.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Microblogs,Multi-label classification,Sentiment analysis},
number = {3},
pages = {1083--1093},
pmid = {25246403},
publisher = {Elsevier Ltd},
title = {{A multi-label classification based approach for sentiment classification}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417414005181},
volume = {42},
year = {2015}
}
@misc{Chen2015b,
abstract = {The discovery of the Higgs boson is remarkable for its importance in modern Physics research. The next step for physicists is to discover more about the Higgs boson from the data of the Large Hadron Collider (LHC). A fundamental and challenging task is to extract the signal of Higgs boson from background noises. The machine learning technique is one important component in solving this problem. In this paper, we propose to solve the Higgs boson classification problem with a gradient boosting approach. Our model learns ensemble of boosted trees that makes careful tradeoff between classification error and model complexity. Physical meaningful features are further extracted to improve the classification accuracy. Our final solution obtained an AMS of 3.71885 on the private leaderboard, making us the top 2{\%} in the Higgs boson challenge.},
author = {Chen, Tianqi and He, Tong},
booktitle = {JMLR: Workshop and Conference Proceedings},
file = {:E$\backslash$:/paper/mendeley/Chen, He - 2015 - Higgs Boson Discovery with Boosted Trees.pdf:pdf},
keywords = {Gradient Boosting,Higgs Boson,Machine Learning},
number = {May 2014},
pages = {69--80},
title = {{Higgs Boson Discovery with Boosted Trees}},
volume = {42},
year = {2015}
}
@article{Valiant1984b,
abstract = {Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learned using it in a reasonable (polynomial) number of steps. Although inherent algorithmic complexity appears to set serious limits to the range of concepts that can be learned, we show that there are some important nontrivial classes of propositional concepts that can be learned in a realistic sense.},
author = {Valiant, L. G.},
doi = {10.1145/1968.1972},
file = {:E$\backslash$:/paper/mendeley/Valiant - 1984 - A theory of the learnable.pdf:pdf},
isbn = {0897911334},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {1134--1142},
pmid = {12929239},
title = {{A theory of the learnable}},
volume = {27},
year = {1984}
}
