Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Min2005b,
abstract = {Bankruptcy prediction has drawn a lot of research interests in previous literature, and recent studies have shown that machine learning techniques achieved better performance than traditional statistical ones. This paper applies support vector machines (SVMs) to the bankruptcy prediction problem in an attempt to suggest a new model with better explanatory power and stability. To serve this purpose, we use a grid-search technique using 5-fold cross-validation to find out the optimal parameter values of kernel function of SVM. In addition, to evaluate the prediction accuracy of SVM, we compare its performance with those of multiple discriminant analysis (MDA), logistic regression analysis (Logit), and three-layer fully connected back-propagation neural networks (BPNs). The experiment results show that SVM outperforms the other methods. ?? 2005 Published by Elsevier Ltd.},
author = {Min, Jae H. and Lee, Young Chan},
doi = {10.1016/j.eswa.2004.12.008},
file = {:E$\backslash$:/paper/mendeley/Min, Lee - 2005 - Bankruptcy prediction using support vector machine with optimal choice of kernel function parameters.pdf:pdf},
isbn = {09574174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Back-propagation neural networks,Bankruptcy prediction,Grid-search,Kernel function,Support vector machine},
number = {4},
pages = {603--614},
title = {{Bankruptcy prediction using support vector machine with optimal choice of kernel function parameters}},
volume = {28},
year = {2005}
}
@article{Min2006b,
abstract = {Bankruptcy prediction is an important and widely studied topic since it can have significant impact on bank lending decisions and profitability. Recently, the support vector machine (SVM) has been applied to the problem of bankruptcy prediction. The SVM-based method has been compared with other methods such as the neural network (NN) and logistic regression, and has shown good results. The genetic algorithm (GA) has been increasingly applied in conjunction with other AI techniques such as NN and Case-based reasoning (CBR). However, few studies have dealt with the integration of GA and SVM, though there is a great potential for useful applications in this area. This study proposes methods for improvingSVMperformance in two aspects: feature subset selection and parameter optimization.GAis used to optimize both a feature subset and parameters of SVM simultaneously for bankruptcy prediction.},
author = {Min, Sung-Hwan and Lee, Jumin and Han, Ingoo},
doi = {10.1016/j.eswa.2005.09.070},
file = {:E$\backslash$:/paper/mendeley/Min, Lee, Han - 2006 - Hybrid genetic algorithms and support vector machines for bankruptcy prediction.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {bankruptcy prediction,genetic algorithms,support vector machines},
number = {3},
pages = {652--660},
title = {{Hybrid genetic algorithms and support vector machines for bankruptcy prediction}},
volume = {31},
year = {2006}
}
@article{Chauhan2009b,
abstract = {In this study, differential evolution algorithm (DE) is proposed to train a wavelet neural network (WNN). The resulting network is named as differential evolution trained wavelet neural network (DEWNN). The efficacy of DEWNN is tested on bankruptcy prediction datasets viz. US banks, Turkish banks and Spanish banks. Further, its efficacy is also tested on benchmark datasets such as Iris, Wine and Wisconsin Breast Cancer. Moreover, Garson's algorithm for feature selection in multi layer perceptron is adapted in the case of DEWNN. The performance of DEWNN is compared with that of threshold accepting trained wavelet neural network (TAWNN) [Vinay Kumar, K., Ravi, V., Mahil Carr, {\&} Raj Kiran, N. (2008). Software cost estimation using wavelet neural networks. Journal of Systems and Software] and the original wavelet neural network (WNN) in the case of all data sets without feature selection and also in the case of four data sets where feature selection was performed. The whole experimentation is conducted using 10-fold cross validation method. Results show that soft computing hybrids viz., DEWNN and TAWNN outperformed the original WNN in terms of accuracy and sensitivity across all problems. Furthermore, DEWNN outscored TAWNN in terms of accuracy and sensitivity across all problems except Turkish banks dataset. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Chauhan, Nikunj and Ravi, V. and {Karthik Chandra}, D.},
doi = {10.1016/j.eswa.2008.09.019},
file = {:E$\backslash$:/paper/mendeley/Chauhan, Ravi, Karthik Chandra - 2009 - Differential evolution trained wavelet neural networks Application to bankruptcy prediction in b.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Bankruptcy prediction,Classification,Differential evolution (DE),Differential evolution trained wavelet neural netw,Threshold accepting trained wavelet neural network,Wavelet neural networks (WNN)},
number = {4},
pages = {7659--7665},
publisher = {Elsevier Ltd},
title = {{Differential evolution trained wavelet neural networks: Application to bankruptcy prediction in banks}},
url = {http://dx.doi.org/10.1016/j.eswa.2008.09.019},
volume = {36},
year = {2009}
}
@article{RaviKumar2007b,
abstract = {This paper presents a comprehensive review of the work done, during the 1968-2005, in the application of statistical and intelligent techniques to solve the bankruptcy prediction problem faced by banks and firms. The review is categorized by taking the type of technique applied to solve this problem as an important dimension. Accordingly, the papers are grouped in the following families of techniques: (i) statistical techniques, (ii) neural networks, (iii) case-based reasoning, (iv) decision trees, (iv) operational research, (v) evolutionary approaches, (vi) rough set based techniques, (vii) other techniques subsuming fuzzy logic, support vector machine and isotonic separation and (viii) soft computing subsuming seamless hybridization of all the above-mentioned techniques. Of particular significance is that in each paper, the review highlights the source of data sets, financial ratios used, country of origin, time line of study and the comparative performance of techniques in terms of prediction accuracy wherever available. The review also lists some important directions for future research. ?? 2006 Elsevier B.V. All rights reserved.},
author = {{Ravi Kumar}, P. and Ravi, V.},
doi = {10.1016/j.ejor.2006.08.043},
file = {:E$\backslash$:/paper/mendeley/Ravi Kumar, Ravi - 2007 - Bankruptcy prediction in banks and firms via statistical and intelligent techniques - A review.pdf:pdf},
isbn = {03772217},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Bankruptcy prediction,Banks,Case-based reasoning,Decision trees,Evolutionary approaches,Firms,Fuzzy logic,Intelligent techniques,Neural networks,Operations research,Rough sets,Statistics,Support vector machine and soft computing},
number = {1},
pages = {1--28},
pmid = {23807594},
title = {{Bankruptcy prediction in banks and firms via statistical and intelligent techniques - A review}},
volume = {180},
year = {2007}
}
@article{Chain2010a,
author = {Chain, Supply and Thomas, Kelly and President, Senior Vice},
file = {:E$\backslash$:/paper/mendeley/Chain, Thomas, President - 2010 - planninG in the ClouD.pdf:pdf},
isbn = {9781479978816},
journal = {Supply Chain Europe},
number = {December},
pages = {7--8},
title = {{planninG in the ClouD}},
year = {2010}
}
@article{Lenstra1977c,
abstract = {We survey and extend the results on the complexity of machine scheduling problems. After a brief review of the central concept of NP-completeness we give a classification of scheduling problems on single, different and identical machines and study the influence of various parameters on their complexity. The problems for which a polynomial-bounded algorithm is available are listed and NP-completeness is established for a large number of other machine scheduling problems. We finally discuss some questions that remain unanswered.},
author = {Lenstra, J.K. and {Rinnooy Kan}, A. H. G. and Brucker, Peter},
doi = {10.1057/jors.1978.179},
file = {:E$\backslash$:/paper/mendeley/Lenstra, Rinnooy Kan, Brucker - 1977 - Complexity of Machine Scheduling Problems.pdf:pdf},
isbn = {9780720407655},
issn = {0160-5682},
journal = {Annals of Discrete Mathematics},
number = {8},
pages = {343--362},
title = {{Complexity of Machine Scheduling Problems}},
volume = {1},
year = {1977}
}
@article{Ghallab2014b,
abstract = {Planning is motivated by acting. Most of the existing work on automated planning underestimates the reasoning and deliberation needed for acting; it is instead biased towards path-finding methods in a compactly specified state-transition system. Researchers in this AI field have developed many planners, but very few actors. We believe this is one of the main causes of the relatively low deployment of automated planning applications. In this paper, we advocate a change in focus to actors as the primary topic of investigation. Actors are not mere plan executors: they may use planning and other deliberation tools, before and during acting. This change in focus entails two interconnected principles: a hierarchical structure to integrate the actor's deliberation functions, and continual online planning and reasoning throughout the acting process. In the paper, we discuss open problems and research directions toward that objective in knowledge representations, model acquisition and verification, synthesis and refinement, monitoring, goal reasoning, and integration. {\textcopyright} 2013 Elsevier B.V.},
author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo},
doi = {10.1016/j.artint.2013.11.002},
file = {:E$\backslash$:/paper/mendeley/Ghallab, Nau, Traverso - 2014 - The actor's view of automated planning and acting A position paper.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Automated planning and acting},
number = {1},
pages = {1--17},
publisher = {Elsevier B.V.},
title = {{The actor's view of automated planning and acting: A position paper}},
url = {http://dx.doi.org/10.1016/j.artint.2013.11.002},
volume = {208},
year = {2014}
}
@article{Fikes1993b,
author = {Fikes, Richard E. and Nilsson, Nils J.},
doi = {10.1016/0004-3702(93)90190-M},
file = {:E$\backslash$:/paper/mendeley/Fikes, Nilsson - 1993 - STRIPS, a retrospective.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
number = {1-2},
pages = {227--232},
title = {{STRIPS, a retrospective}},
volume = {59},
year = {1993}
}
@article{Domingos2012b,
abstract = {MACHINE LEARNING SYSTEMS automatically learn programs from data. This is often a very attractive alternative to manually constructing them, and in the last decade the use of machine learning has spread rapidly throughout computer science and beyond. Machine learning is used in Web search, spam filters, recommender systems, ad placement, credit scoring, fraud detection, stock trading, drug design, and many other applications. A recent report from the McKinsey Global Institute asserts that machine learning (a.k.a. data mining or predictive analytics) will be the driver of the next big wave of innovation. Several fine textbooks are available to interested practitioners and researchers (for example, Mitchell and Witten et al.). However, much of the “folk knowledge” that is needed to successfully develop machine learning applications is not readily available in them. As a result, many machine learning projects take much longer than necessary or wind up producing less-than-ideal results. Yet much of this folk knowledge is fairly easy to communicate. This is the purpose of this article.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
eprint = {9605103},
file = {:E$\backslash$:/paper/mendeley/Domingos - 2012 - A few useful things to know about machine learning(4).pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pages = {78},
pmid = {1000183096},
primaryClass = {cs},
title = {{A few useful things to know about machine learning}},
volume = {55},
year = {2012}
}
@article{Fikes1971b,
abstract = {We describe a new problem solver called STRIPS that attempts to find a sequence of operators in a space of world models to transform a given initial world model in which a given goal formula can be proven to be true. STRIPS represents a world model as an arbitrary collection in first-order predicate calculus formulas and is designed to work with models consisting of large numbers of formula. It employs a resolution theorem prover to answer questions of particular models and uses means-ends analysis to guide it to the desired goal-satisfying model. ?? 1971.},
author = {Fikes, Richard E. and Nilsson, Nils J.},
doi = {10.1016/0004-3702(71)90010-5},
file = {:E$\backslash$:/paper/mendeley/Fikes, Nilsson - 1971 - Strips A new approach to the application of theorem proving to problem solving.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Problem solving,heuristic search,robot planning,theorem proving},
number = {3-4},
pages = {189--208},
title = {{Strips: A new approach to the application of theorem proving to problem solving}},
volume = {2},
year = {1971}
}
@article{McDermott1998b,
abstract = {This manual describes the syntax of PDDL, the Planning Domain Definition Language, the problem-specification language for the AIPS-98 planning competition. The language has roughly the the expressiveness of Pednault's ADL 10 for propositions, and roughly the expressiveness of UMCP 6 for actions. Our hope is to encourage empirical evaluation of planner performance, and development of standard sets of problems all in comparable notations.},
author = {McDermott, Drew and Ghallab, Malik and Howe, Adele and Knoblock, Craig and Ram, Ashwin and Veloso, Manuela and Weld, Daniel and Wilkins, David},
doi = {10.1.1.37.212},
file = {:E$\backslash$:/paper/mendeley/McDermott et al. - 1998 - PDDL - The Planning Domain Definition Language.pdf:pdf},
isbn = {CVC TR-980003/DCS TR-1165},
journal = {Annals of Physics},
pages = {26},
title = {{PDDL - The Planning Domain Definition Language}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.212},
volume = {54},
year = {1998}
}
@article{Schmidhuber2015a,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J??rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:E$\backslash$:/paper/mendeley/Schmidhuber - 2015 - Deep Learning in neural networks An overview.pdf:pdf},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
pages = {85--117},
pmid = {25462637},
title = {{Deep Learning in neural networks: An overview}},
volume = {61},
year = {2015}
}
@article{Cushing2007c,
author = {Cushing, William and Kambhampati, Subbarao and Mausam and Weld, Daniel S.},
file = {:E$\backslash$:/paper/mendeley/Cushing et al. - 2007 - When is temporal planning really temporal.pdf:pdf},
isbn = {1267826835},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
pages = {1852--1859},
title = {{When is temporal planning really temporal?}},
year = {2007}
}
@article{Statistics2017b,
author = {Statistics, Mathematical},
file = {:E$\backslash$:/paper/mendeley/Statistics - 2017 - Greedy Function Approximation A Gradient Boosting Machine Author ( s ) Jerome H . Friedman Source The Annals of St.pdf:pdf},
number = {5},
pages = {1189--1232},
title = {{Greedy Function Approximation : A Gradient Boosting Machine Author ( s ): Jerome H . Friedman Source : The Annals of Statistics , Vol . 29 , No . 5 ( Oct ., 2001 ), pp . 1189-1232 Published by : Institute of Mathematical Statistics Stable URL : http://www}},
volume = {29},
year = {2017}
}
@book{Russellb,
author = {Russell, Stuart J. and Peter, Norving},
file = {:E$\backslash$:/paper/mendeley/Russell, Peter - Unknown - Artificial Intelligence A Modern Approach 3rd.pdf.pdf:pdf},
isbn = {9780136042594},
title = {{Artificial Intelligence A Modern Approach 3rd.pdf}}
}
@article{Chih-WeiHsuChih-ChungChang2008b,
abstract = {The support vector machine (SVM) is a popular classi cation technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but signi cant steps. In this guide, we propose a simple procedure which usually gives reasonable results. developed well-differentiated superficial transitional cell bladder cancer. CONCLUSIONS: Patients with SCI often prefer SPC than other methods offered to them, because of quality-of-life issues. The incidence of significant complications might not be as high as previously reported, and with a commitment to careful follow-up, SPC can be a safe option for carefully selected patients if adequate surveillance can be ensured.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{Chih-Wei Hsu, Chih-Chung Chang}, and Chih-Jen Lin},
doi = {10.1177/02632760022050997},
eprint = {0-387-31073-8},
file = {:E$\backslash$:/paper/mendeley/Chih-Wei Hsu, Chih-Chung Chang - 2008 - A Practical Guide to Support Vector Classification.pdf:pdf},
isbn = {013805326X},
issn = {1464-410X},
journal = {BJU international},
number = {1},
pages = {1396--400},
pmid = {18190633},
title = {{A Practical Guide to Support Vector Classification}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/papers/guide/guide.pdf},
volume = {101},
year = {2008}
}
@article{Kim2015b,
abstract = {In classification or prediction tasks, data imbalance problem is frequently observed when most of instances belong to one majority class. Data imbalance problem has received considerable attention in machine learning community because it is one of the main causes that degrade the performance of classifiers or predictors. In this paper, we propose geometric mean based boosting algorithm (GMBoost) to resolve data imbalance problem. GMBoost enables learning with consideration of both majority and minority classes because it uses the geometric mean of both classes in error rate and accuracy calculation. To evaluate the performance of GMBoost, we have applied GMBoost to bankruptcy prediction task. The results and their comparative analysis with AdaBoost and cost-sensitive boosting indicate that GMBoost has the advantages of high prediction power and robust learning capability in imbalanced data as well as balanced data distribution.},
author = {Kim, Myoung Jong and Kang, Dae Ki and Kim, Hong Bae},
doi = {10.1016/j.eswa.2014.08.025},
file = {:E$\backslash$:/paper/mendeley/Kim, Kang, Kim - 2015 - Geometric mean based boosting algorithm with over-sampling to resolve data imbalance problem for bankruptcy pred.pdf:pdf},
isbn = {9781612082479},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {AdaBoost,Bankruptcy prediction,Cost-sensitive boosting,Data imbalance,GMBoost,Over-sampling,SMOTE},
number = {3},
pages = {1074--1082},
publisher = {Elsevier Ltd},
title = {{Geometric mean based boosting algorithm with over-sampling to resolve data imbalance problem for bankruptcy prediction}},
url = {http://dx.doi.org/10.1016/j.eswa.2014.08.025},
volume = {42},
year = {2015}
}
@article{Hempelmann2013a,
abstract = {The paper briefly discusses techniques of semi-automatic detection of outdated knowledge within the framework of Ontological Semantic Technology. Following an overview of the architecture and functionality of Ontological Semantic Technology, the paper illustrates, based on specific examples, how subsumption gaps are detected, and historical and evolving knowledge is incorporated into the ontology.},
author = {Hempelmann, Christian F. and Petrenko, Max},
doi = {10.1109/ICSC.2013.29},
file = {:E$\backslash$:/paper/mendeley/Hempelmann, Petrenko - 2013 - Knowledge Representation and Adaptation within Ontological Semantic Technology.pdf:pdf},
isbn = {978-0-7695-5119-7},
journal = {2013 IEEE Seventh International Conference on Semantic Computing},
keywords = {Ontological Semantic Technology,adaptation,ontology,ontology evaluation,ontology learning,unattested input},
pages = {114--117},
title = {{Knowledge Representation and Adaptation within Ontological Semantic Technology}},
url = {http://dl.acm.org/citation.cfm?id=2571268.2571313},
year = {2013}
}
@article{Zhou2013b,
abstract = {Corporate bankruptcy prediction is very important for creditors and investors. Most literature improves performance of prediction models by developing and optimizing the quantitative methods. This paper investigates the effect of sampling methods on the performance of quantitative bankruptcy prediction models on real highly imbalanced dataset. Seven sampling methods and five quantitative models are tested on two real highly imbalanced datasets. A comparison of model performance tested on random paired sample set and real imbalanced sample set is also conducted. The experimental results suggest that the proper sampling method in developing prediction models is mainly dependent on the number of bankruptcies in the training sample set. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Zhou, Ligang},
doi = {10.1016/j.knosys.2012.12.007},
file = {:E$\backslash$:/paper/mendeley/Zhou - 2013 - Performance of corporate bankruptcy prediction models on imbalanced dataset The effect of sampling methods.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Bankruptcy prediction,Classification,Imbalanced dataset,Oversampling,Undersampling},
pages = {16--25},
title = {{Performance of corporate bankruptcy prediction models on imbalanced dataset: The effect of sampling methods}},
url = {http://dx.doi.org/10.1016/j.knosys.2012.12.007},
volume = {41},
year = {2013}
}
@article{Schapire1999b,
abstract = {Boosting is a general method for improving the accuracy of any given learning algorithm. This short overview paper introduces the boosting algorithm AdaBoost, and explains the underlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting's relationship to support-vector machines. Some examples of recent applications of boosting are also described.},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.01136v1},
author = {Schapire, Robert E.},
doi = {citeulike-article-id:765005},
eprint = {arXiv:1508.01136v1},
file = {:E$\backslash$:/paper/mendeley/Schapire - 1999 - A brief introduction to boosting.pdf:pdf},
isbn = {3540440119},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
number = {5},
pages = {1401--1406},
title = {{A brief introduction to boosting}},
volume = {2},
year = {1999}
}
@article{He2009b,
abstract = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {He, Haibo and Garcia, Edwardo A.},
doi = {10.1109/TKDE.2008.239},
eprint = {arXiv:1011.1669v3},
file = {:E$\backslash$:/paper/mendeley/He, Garcia - 2009 - Learning from imbalanced data.pdf:pdf},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Active learning,Assessment metrics,Classification,Cost-sensitive learning,Imbalanced learning,Kernel-based learning,Sampling methods},
number = {9},
pages = {1263--1284},
pmid = {24807526},
title = {{Learning from imbalanced data}},
volume = {21},
year = {2009}
}
@book{Bash2015a,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bash, Eleanor},
booktitle = {PhD Proposal},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/shuoshuo/Desktop/Artificial Intelligence A Modern ApproachContent.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
pmid = {25246403},
title = {{No Title No Title}},
volume = {1},
year = {2015}
}
@article{Yoona,
author = {Yoon, Young and Robinson, Nathan and Muthusamy, Vinod and Mcllraith, Sheila and Jacobsen, Hans-Arno},
file = {:E$\backslash$:/paper/mendeley/Yoon et al. - Unknown - Planning the Transformation of Distributed Messaging Middlewares.pdf:pdf},
isbn = {9781450320658},
keywords = {overlay networks,publish,reliability,subscribe},
pages = {4--6},
title = {{Planning the Transformation of Distributed Messaging Middlewares}}
}
@article{Guyon2003b,
abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pre- dictors, providing faster andmore cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
doi = {10.1016/j.aca.2011.07.027},
eprint = {1111.6189v1},
file = {:E$\backslash$:/paper/mendeley/Guyon, Elisseeff - 2003 - An Introduction to Variable and Feature Selection(2).pdf:pdf},
isbn = {0885-6125},
issn = {00032670},
journal = {Journal of Machine Learning Research (JMLR)},
keywords = {Biochemical oxygen demand,Kernel discriminant analysis,Kernel partial least squares,Support vector classification,Support vector regression,Water quality},
number = {3},
pages = {1157--1182},
pmid = {21889629},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
@article{Borrajo2015a,
author = {Borrajo, Daniel and Roub{\'{i}}{\v{c}}kov{\'{a}}, Anna and Serina, Ivan},
doi = {10.1145/2674024},
file = {:E$\backslash$:/paper/mendeley/Borrajo, Roub{\'{i}}{\v{c}}kov{\'{a}}, Serina - 2015 - Progress in Case-Based Planning.pdf:pdf},
issn = {0360-0300},
journal = {ACM Comput. Surv.},
keywords = {Case-based planning,automated planning},
number = {2},
pages = {35:1----35:39},
title = {{Progress in Case-Based Planning}},
url = {http://doi.acm.org/10.1145/2674024},
volume = {47},
year = {2015}
}
@article{Chen2016b,
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end- to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quan- tile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compres- sion and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
archivePrefix = {arXiv},
arxivId = {1603.02754},
author = {Chen, Tianqi and Guestrin, Carlos},
doi = {10.1145/2939672.2939785},
eprint = {1603.02754},
file = {:E$\backslash$:/paper/mendeley/Chen, Guestrin - 2016 - XGBoost Reliable Large-scale Tree Boosting System.pdf:pdf},
isbn = {9781450342322},
issn = {0146-4833},
journal = {arXiv},
keywords = {large-scale machine learning},
pages = {1--6},
title = {{XGBoost : Reliable Large-scale Tree Boosting System}},
year = {2016}
}
@article{Laborie2003d,
abstract = {This paper summarizes the main existing approaches to propagate resource constraints in Constraint-Based scheduling and identifies some of their limitations for using them in an integrated planning and scheduling framework. We then describe two new algorithms to propagate resource constraints on discrete resources and reservoirs. Unlike most of the classical work in scheduling, our algorithms focus on the precedence relations between activities rather than on their absolute position in time. They are efficient even when the set of activities is not completely defined and when the time window of activities is large. These features explain why our algorithms are particularly suited for integrated planning and scheduling approaches. All our algorithms are illustrated with examples. Encouraging preliminary results are reported on pure scheduling problems as well as some possible extensions of our framework. ?? 2002 Elsevier Science B.V. All rights reserved.},
author = {Laborie, Philippe},
doi = {10.1016/S0004-3702(02)00362-4},
file = {:E$\backslash$:/paper/mendeley/Laborie - 2003 - Algorithms for propagating resource constraints in AI planning and scheduling Existing approaches and new results.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {AI planning,Constraint programming,Cumulative resources,Scheduling},
number = {2},
pages = {151--188},
pmid = {2976},
title = {{Algorithms for propagating resource constraints in AI planning and scheduling: Existing approaches and new results}},
volume = {143},
year = {2003}
}
@article{Wang2014b,
abstract = {With the recent financial crisis and European debt crisis, corporate bankruptcy prediction has become an increasingly important issue for financial institutions. Many statistical and intelligent methods have been proposed, however, there is no overall best method has been used in predicting corporate bankruptcy. Recent studies suggest ensemble learning methods may have potential applicability in corporate bankruptcy prediction. In this paper, a new and improved Boosting, FS-Boosting, is proposed to predict corporate bankruptcy. Through injecting feature selection strategy into Boosting, FS-Booting can get better performance as base learners in FS-Boosting could get more accuracy and diversity. For the testing and illustration purposes, two real world bankruptcy datasets were selected to demonstrate the effectiveness and feasibility of FS-Boosting. Experimental results reveal that FS-Boosting could be used as an alternative method for the corporate bankruptcy prediction. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Wang, Gang and Ma, Jian and Yang, Shanlin},
doi = {10.1016/j.eswa.2013.09.033},
file = {:E$\backslash$:/paper/mendeley/Wang, Ma, Yang - 2014 - An improved boosting based on feature selection for corporate bankruptcy prediction.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Boosting,Corporate bankruptcy prediction,Ensemble learning,Feature selection},
number = {5},
pages = {2353--2361},
publisher = {Elsevier Ltd},
title = {{An improved boosting based on feature selection for corporate bankruptcy prediction}},
url = {http://dx.doi.org/10.1016/j.eswa.2013.09.033},
volume = {41},
year = {2014}
}
@book{Pinedo2012d,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Pinedo, Michael L},
booktitle = {Springer},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {9809069v1},
file = {:E$\backslash$:/paper/mendeley/Pinedo - 2012 - Scheduling Theoty, Algotithms, and Systems.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
keywords = {Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\'{e}}lib{\'{e}}r{\'{e}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics {\&} numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,JUVENTUD,MODIFICACIONES CORPORALES,Male,Motivation,Movement,Risk-Taking,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics {\&} numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,aesthetics,and on cor-,as none were found,autoinjury and health,body,complications did not,complications from inserting a,constituci{\'{o}}n del yo,control postural- estabilizaci{\'{o}}n- v{\'{i}}as,corporal modifications,corps,cuerpo,culturas juveniles,cultures juv{\'{e}}niles,epidural,esth{\'{e}}tique,est{\'{e}}tica,find any reports of,high resolution images,if neuraxial anes-,ing with neuraxial anesthesia,jeunesse,juvenile cultures,juventud,mecanismos de anteroalimentaci{\'{o}}n y,modificacio -,needle through a,nes corporales,perforaci{\'{o}}n corporal,piel,pr{\'{a}}ctica autolesiva,psicoan{\'{a}}lisis,research,retroalimentaci{\'{o}}n,risks management,segunda piel,sensitivas y motoras,spinal,sustainable reconstruction,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,was reviewed to see,youth},
pages = {1--673},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Scheduling: Theoty, Algotithms, and Systems}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15003161{\%}5Cnhttp://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991{\%}5Cnhttp://www.scielo.cl/pdf/udecada/v15n26/art06.pdf{\%}5Cnhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84861150233{\&}partnerID=tZOtx3y1},
volume = {4},
year = {2012}
}
@article{Descotte1985c,
abstract = {Process planning consists in producing plans for manufacturing parts. It requires taking into account a great variety of pieces of knowledge. Most of them are not pure constraints, but preferences between which compromises are necessary. This is a situation which is typical of many real-life planning tasks. We have designed a planner which operates by iteratively constraining a loosely constrained initial plan built from the model of the part to be machined. If an inconsistent (i.e. overconstrained) plan is produced, then the planner makes use of a selective backtracking procedure which reasons about the weights attached to the applied constraints for deciding which compromise is 'best'. Successively applied constraints are drawn from 'pieces of advice' provided by an expert-type knowledge base. A system, gari, embedding this planner has been implemented and experimented for generating the machining plans of mechanical parts. Some plans were produced after several hundred constraints were applied, among which more than one hundred compromises were necessary. ?? 1985.},
author = {Descotte, Yannick and Latombe, Jean Claude},
doi = {10.1016/0004-3702(85)90053-0},
file = {:E$\backslash$:/paper/mendeley/Descotte, Latombe - 1985 - Making compromises among antagonist constraints in a planner.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
number = {2},
pages = {183--217},
title = {{Making compromises among antagonist constraints in a planner}},
volume = {27},
year = {1985}
}
@article{Johnston1992c,
abstract = {Creating an optimum long-term schedule for the Hubble Space Telescope is difficult by almost any standard due to the large number of activities, many relative and absolute time constraints, prevailing uncertainties and an unusually wide range of timescales. This problem has motivated research in neural networks for scheduling. The novel concept of continuous suitaility functions defined over a continuous time domain has been developed to represent soft temporal relationships between activities. All constraints and preferences are automatically translated into the weights of an appropriately designed artificial neural network. The constraints are subject to propagation and consistency enhancement in order to increase the number of explicitly represented constraints. Equipped with a novel stochastic neuron update rule, the resulting GDS-network effectively implements a Las Vegas-type algorithm to generate good schedules with an unparalleled efficiency. When provided with feedback from execution the network allows dynamic schedule revision and repair. ?? 1992.},
author = {Johnston, M. D. and Adorf, H. M.},
doi = {10.1016/0305-0548(92)90045-7},
file = {:E$\backslash$:/paper/mendeley/Johnston, Adorf - 1992 - Scheduling with neural networks-the case of the hubble space telescope.pdf:pdf},
issn = {03050548},
journal = {Computers and Operations Research},
number = {3-4},
pages = {209--240},
title = {{Scheduling with neural networks-the case of the hubble space telescope}},
volume = {19},
year = {1992}
}
@article{McDermott2000b,
abstract = {The 1998 Planning Competition at the AI Planning Systems Conference was the first of its kind. Its goal was to create planning domains that a wide variety of planning researchers could agree on to make comparison among planners more meaningful, measure overall progress in the field, and set up a framework for long-term creation of a repository of problems in a standard notation. A rules committee for the competition was created in 1997 and had long discussions on how the contest should go. One result of these discussions was the pddl notation for planning domains. This notation was used to set up a set of planning problems and get a modest problem repository started. As a result, five planning systems were able to compete when the contest took place in June 1998. All these systems solved problems in the strips framework, with some slight extensions. The attempt to find domains for other forms of planning foundered because of technical and organizational problems. In spite of this problem, the competition achieved its goals partially in that it confirmed that substantial progress had occurred in some subfields of planning, and it allowed qualitative comparison among different planning algorithms. It is urged that the competition continue to take place and to evolve.},
author = {McDermott, Dm},
file = {:E$\backslash$:/paper/mendeley/McDermott - 2000 - The 1998 AI planning systems competition.pdf:pdf},
isbn = {07384602},
issn = {07384602},
journal = {AI magazine},
number = {2},
pages = {1--33},
title = {{The 1998 AI planning systems competition}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/1506},
volume = {21},
year = {2000}
}
@article{Nanni2009b,
abstract = {In this paper, we investigate the performance of several systems based on ensemble of classifiers for bankruptcy prediction and credit scoring. The obtained results are very encouraging, our results improved the performance obtained using the stand-alone classifiers. We show that the method "Random Subspace" outperforms the other ensemble methods tested in this paper. Moreover, the best stand-alone method is the multi-layer perceptron neural net, while the best method tested in this work is the Random Subspace of Levenberg-Marquardt neural net. In this work, three financial datasets are chosen for the experiments: Australian credit, German credit, and Japanese credit. ?? 2008 Elsevier Ltd. All rights reserved.},
author = {Nanni, Loris and Lumini, Alessandra},
doi = {10.1016/j.eswa.2008.01.018},
file = {:E$\backslash$:/paper/mendeley/Nanni, Lumini - 2009 - An experimental comparison of ensemble of classifiers for bankruptcy prediction and credit scoring.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Bankruptcy prediction,Credit scoring,Ensemble of classifiers},
number = {2 PART 2},
pages = {3028--3033},
publisher = {Elsevier Ltd},
title = {{An experimental comparison of ensemble of classifiers for bankruptcy prediction and credit scoring}},
url = {http://dx.doi.org/10.1016/j.eswa.2008.01.018},
volume = {36},
year = {2009}
}
@article{Foxc,
author = {Fox, S and Allen, Brad and Strohm, Gary},
file = {:E$\backslash$:/paper/mendeley/Fox, Allen, Strohm - Unknown - Job-Shop Scheduling An.pdf:pdf},
pages = {155--158},
title = {{Job-Shop Scheduling: An}}
}
@article{McCalla1983a,
author = {McCalla, Gordon and Cercone, Nick},
doi = {10.1109/MC.1983.1654192},
file = {:E$\backslash$:/paper/mendeley/McCalla, Cercone - 1983 - Guest Editors' Introduction Approaches to Knowledge Representation.pdf:pdf},
issn = {00189162},
journal = {IEEE Computer},
pages = {12--18},
title = {{Guest Editors' Introduction : Approaches to Knowledge Representation}},
volume = {83},
year = {1983}
}
@article{Liu2015b,
abstract = {A multi-label classification based approach for sentiment analysis is proposed in this paper. To the best of our knowledge, this work is the first to propose to use multi-label classification for sentiment classification of microblogs. The proposed prototype has three main components, text segmentation, feature extraction, and multi-label classification. Raw segmented words and sentiment features based on the three different sentiment dictionaries, Dalian University of Technology Sentiment Dictionary, National Taiwan University Sentiment Dictionary and HowNet Dictionary, are the features and the bag of words is the feature representation. A detailed empirical study of different multi-label classification methods on sentiment classification is conducted to compare their classification performances. Specifically, total 11 state of the art multi-label classification methods are compared on two microblog datasets and 8 evaluation metrics are used. The effects of the three sentiment dictionaries for multi-label classification are empirically studied and compared, which, to the best of our knowledge, have not been performed. The performed empirical comparisons show that Dalian University of Technology Sentiment Dictionary has the best performance among the three different sentiment dictionaries.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Liu, Shuhua Monica and Chen, Jiun-Hung},
doi = {10.1016/j.eswa.2014.08.036},
eprint = {arXiv:1011.1669v3},
file = {:E$\backslash$:/paper/mendeley/Liu, Chen - 2015 - A multi-label classification based approach for sentiment classification.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Microblogs,Multi-label classification,Sentiment analysis},
number = {3},
pages = {1083--1093},
pmid = {25246403},
publisher = {Elsevier Ltd},
title = {{A multi-label classification based approach for sentiment classification}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417414005181},
volume = {42},
year = {2015}
}
@article{Muscettola2002d,
abstract = {Computing tight resource-level bounds is a fundamental problem in the construction of flexible plans with resource utilization. In this paper we describe an efficient algorithm that builds a resource envelope, the tightest possible such bound. The algorithm is based on transforming the temporal network of resource consuming and producing events into a flow network with nodes equal to the events and edges equal to the necessary predecessor links between events. A staged maximum flow problem on the network is then used to compute the time of occurrence and the height of each step of the resource envelope profile. Each stage has the same computational complexity of solving a maximum flow problem on the entire flow network. This makes this method computationally feasible and promising for use in the inner loop of flexible-time scheduling algorithms.},
author = {Muscettola, Nicola},
doi = {10.1007/3-540-46135-3_10},
file = {:E$\backslash$:/paper/mendeley/Muscettola - 2002 - Computing the Envelope for Stepwise-Constant Resource Allocations.pdf:pdf},
isbn = {3-540-44120-4},
issn = {16113349},
journal = {Principles and Practice of Constraint Programming},
pages = {139--154},
title = {{Computing the Envelope for Stepwise-Constant Resource Allocations}},
year = {2002}
}
@misc{Chen2015b,
abstract = {The discovery of the Higgs boson is remarkable for its importance in modern Physics research. The next step for physicists is to discover more about the Higgs boson from the data of the Large Hadron Collider (LHC). A fundamental and challenging task is to extract the signal of Higgs boson from background noises. The machine learning technique is one important component in solving this problem. In this paper, we propose to solve the Higgs boson classification problem with a gradient boosting approach. Our model learns ensemble of boosted trees that makes careful tradeoff between classification error and model complexity. Physical meaningful features are further extracted to improve the classification accuracy. Our final solution obtained an AMS of 3.71885 on the private leaderboard, making us the top 2{\%} in the Higgs boson challenge.},
author = {Chen, Tianqi and He, Tong},
booktitle = {JMLR: Workshop and Conference Proceedings},
file = {:E$\backslash$:/paper/mendeley/Chen, He - 2015 - Higgs Boson Discovery with Boosted Trees.pdf:pdf},
keywords = {Gradient Boosting,Higgs Boson,Machine Learning},
number = {May 2014},
pages = {69--80},
title = {{Higgs Boson Discovery with Boosted Trees}},
volume = {42},
year = {2015}
}
@article{Peer2005a,
abstract = {This article gives an overview of AI (Artificial Intelligence) plan-ning techniques and discusses their application to the Web service composition problem.},
author = {Peer, Joachim},
file = {:E$\backslash$:/paper/mendeley/Peer - 2005 - Web Service Composition as AI Planning – a Survey.pdf:pdf},
journal = {Language},
number = {March},
pages = {63},
title = {{Web Service Composition as AI Planning – a Survey}},
year = {2005}
}
@article{Lawler1973c,
abstract = {Suppose n jobs are each to be processed by a single machine, subject to arbitrary given precedence constraints. Associated with each job j is a known processing time a[SUBj] and a monotone nondecreasing cost function c[SUBj](t), giving the cost that is incurred by the completion of that job at time t. The problem is to find a sequence which will minimize the maximum of the incurred costs. An efficient computational procedure is given for this problem, generalizing and simplifying previous results of the present author and J. M. Moore. [ABSTRACT FROM AUTHOR]},
author = {Lawler, E. L.},
doi = {10.1287/mnsc.19.5.544},
file = {:E$\backslash$:/paper/mendeley/Lawler - 1973 - Optimal Sequencing of a Single Machine Subject to Precedence Constraints(3).pdf:pdf},
issn = {0025-1909},
journal = {Management Science},
number = {5},
pages = {544--546},
title = {{Optimal Sequencing of a Single Machine Subject to Precedence Constraints}},
volume = {19},
year = {1973}
}
@article{Lavalle2006a,
abstract = {This book presents a unified treatment of many different kinds of planning algorithms. The subject lies at the crossroads between robotics, control theory, artificial intelligence, algorithms, and computer graphics. The particular subjects covered include motion planning, discrete planning, planning under uncertainty, sensor-based planning, visibility, decision-theoretic planning, game theory, information spaces, reinforcement learning, nonlinear systems, trajectory planning, nonholonomic planning, and kinodynamic planning.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Lavalle, Steven M},
doi = {10.1017/CBO9780511546877},
eprint = {arXiv:1011.1669v3},
file = {:E$\backslash$:/paper/mendeley/Lavalle - 2006 - Planning Algorithms.pdf:pdf},
isbn = {9780511546877},
issn = {1098-6596},
journal = {Cambridge},
pages = {842},
pmid = {25246403},
title = {{Planning Algorithms}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511546877},
year = {2006}
}
@article{Fox1990c,
author = {Fox, M S},
file = {:E$\backslash$:/paper/mendeley/Fox - 1990 - Constraint-guided scheduling - A short history of research at CMU.pdf:pdf},
journal = {Computers in Industry},
keywords = {Scheduling},
pages = {79--88},
title = {{Constraint-guided scheduling - A short history of research at CMU}},
volume = {14},
year = {1990}
}
@article{Borrajo2013a,
abstract = {Generating plans for a single agent has been shown to be a difficult task. If we generalize to a multi-agent setting, the problem becomes exponentially harder in general. The centralized approach where a plan is jointly generated for all agents is only possible in some applications when agents do not have private goals, actions or states. We describe in this paper an alternative approach, mapr (Multi-Agent Planning by plan Reuse), that considers both the agents private and public information. We have been inspired by iterative Multi-Agent Planning (MAP) techniques as the one presented in [3]. mapr first assigns a subset of public goals to each agent, while each agent might have a set of private goals also. Then, mapr calls the first agent to provide a solution (plan) that takes into account its private and public goals. mapr iteratively calls each agent with the solutions provided by previous agents. Each agent receives its own goals plus the goals of the previous agents. Thus, each agent solves its problem, but taking into account the previous agents solutions. Since previous solutions might consider private data, all private information from an agent is obfuscated for the next ones. Since each agent receives the plan from the previous agent that implicitly considers the solutions to all previous agents, instead of starting the search from scratch, it can also reuse the previous whole plan or only a subset of the actions. Experiments show that mapr outperforms in several orders of magnitude state-of-the-art techniques in the tested domains.},
author = {Borrajo, Daniel},
file = {:E$\backslash$:/paper/mendeley/Borrajo - 2013 - Multi-Agent Planning by Plan Reuse. {\{}E{\}}xtended abstract.pdf:pdf},
journal = {Proceedings of the AAMAS'13},
keywords = {automated planning,multi-agent planning},
pages = {1141--1142},
title = {{Multi-Agent Planning by Plan Reuse. {\{}E{\}}xtended abstract}},
year = {2013}
}
@article{Weld1999a,
abstract = {The past five years have seen dramatic advances in planning algorithms, with an emphasis on propositional methods such as GRAPHPLAN and compilers that convert planning problems into propositional conjunctive normal form formulas for solution using systematic or stochastic SAT methods. Related work, in the context of spacecraft control, advances our understanding of interleaved planning and execution. In this survey, I explain the latest techniques and suggest areas for future research.},
author = {Weld, Daniel S.},
doi = {10.1609/aimag.v20i2.1459},
file = {:E$\backslash$:/paper/mendeley/Weld - 1999 - Recent Advances in AI Planning.pdf:pdf},
isbn = {3540639128},
issn = {0738-4602},
journal = {AI Magazine},
number = {2},
pages = {93},
title = {{Recent Advances in AI Planning}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/view/1459},
volume = {20},
year = {1999}
}
@article{Huang2011a,
abstract = {Knowledge plays a very important role in remote sensing image understanding. In this paper, we consider various types of knowledge related to remote sensing image understanding, and present a knowledge representation(KR) architecture. Knowledge in the KR architecture is classified into six types, including object knowledge, image knowledge, environment knowledge, algorithm knowledge, task knowledge, and integrated knowledge, which combine knowledge from symbolic representations and computational intelligence. We analysis each knowledge type and its representation, especially task knowledge and integrated knowledge. We employ agents for task knowledge representation, which are able to finish special tasks. Meanwhile, task agents bridge the gap between low-level image processing methods and high-level semantic descriptions. The KR architecture provides the basis of knowledge services for remote sensing image understanding systems.},
author = {Huang, Gaopan and Tian, Yuan and Chang, Guanqing},
doi = {10.1109/ITAIC.2011.6030186},
file = {:E$\backslash$:/paper/mendeley/Huang, Tian, Chang - 2011 - A knowledge representation architecture for remote sensing image understanding systems.pdf:pdf},
isbn = {978-1-4244-8622-9},
journal = {2011 6th IEEE Joint International Information Technology and Artificial Intelligence Conference},
keywords = {Computer architecture,Inference algorithms,Knowledge representation,Meteorology,Remote sensing,Semantics,algorithm knowledge,computational intelligence,environment knowledge,high-level semantic descriptions,image knowledge,image processing,image understanding,integrated knowledge,knowledge representation,knowledge representation architecture,low-level image processing methods,object knowledge,remote sensing,remote sensing image understanding systems,symbolic representations,task agent,task knowledge},
pages = {202--205},
title = {{A knowledge representation architecture for remote sensing image understanding systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6030186},
volume = {1},
year = {2011}
}
@article{Fallis2013c,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:E$\backslash$:/paper/mendeley/Fallis - 2013 - No Title No Title.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{No Title No Title}},
volume = {53},
year = {2013}
}
@article{Muscettola1998c,
abstract = {Renewed motives for space exploration have inspired {\{}NASA{\}} to work$\backslash$ntoward the goal of establishing a virtual presence in space, through$\backslash$nheterogeneous fleets of robotic explorers. Information technology,$\backslash$nand Artificial Intelligence in particular, will play a central role$\backslash$nin this endeavor by endowing these explorers with a form of computational$\backslash$nintelligence that we call remote agents. In this paper we describe$\backslash$nthe Remote Agent, a specific autonomous agent architecture based$\backslash$non the principles of model-based programming, on-board deduction$\backslash$nand search, and goal-directed closed-loop commanding, that takes$\backslash$na significant step toward enabling this future. This architecture$\backslash$naddresses the unique characteristics of the spacecraft domain that$\backslash$nrequire highly reliable autonomous operations over long periods of$\backslash$ntime with tight deadlines, resource constraints, and concurrent activity$\backslash$namong tightly coupled subsystems. The Remote Agent integrates constraintbased$\backslash$ntemporal planning and scheduling, robust multi-threaded execution,$\backslash$nand model-based mode identification and reconfiguration. The demonstration$\backslash$nof the integrated system as an on-board controller for Deep Space$\backslash$nOne, {\{}NASA's{\}} first New Millennium mission, is scheduled for a period$\backslash$nof a week in mid 1999. The development of the Remote Agent also provided$\backslash$nthe opportunity to reassess some of {\{}AI's{\}} conventional wisdom about$\backslash$nthe challenges of implementing embedded systems, tractable reasoning,$\backslash$nand knowledge representation. We discuss these issues, and our often$\backslash$ncontrary experiences, throughout the paper.},
author = {Muscettola, Nicola and Nayak, P.Pandurang and Pell, Barney and Williams, Brian C.},
doi = {10.1016/S0004-3702(98)00068-X},
file = {:E$\backslash$:/paper/mendeley/Muscettola et al. - 1998 - Remote Agent to boldly go where no AI system has gone before.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {architectures,autonomous agents,constraint-based planning,scheduling},
number = {1-2},
pages = {5--47},
title = {{Remote Agent: to boldly go where no AI system has gone before}},
volume = {103},
year = {1998}
}
@article{Valiant1984b,
abstract = {Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learned using it in a reasonable (polynomial) number of steps. Although inherent algorithmic complexity appears to set serious limits to the range of concepts that can be learned, we show that there are some important nontrivial classes of propositional concepts that can be learned in a realistic sense.},
author = {Valiant, L. G.},
doi = {10.1145/1968.1972},
file = {:E$\backslash$:/paper/mendeley/Valiant - 1984 - A theory of the learnable.pdf:pdf},
isbn = {0897911334},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {1134--1142},
pmid = {12929239},
title = {{A theory of the learnable}},
volume = {27},
year = {1984}
}
